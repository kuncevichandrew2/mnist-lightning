{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import torch \n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1337\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "SEED_CONST = 1337\n",
    "\n",
    "def set_seed(seed=SEED_CONST):\n",
    "    \"\"\"Set all seeds into the project!\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "   \n",
    "\n",
    "    # torch.cuda.manual_so_all(seed)  # for multi-GPU setups\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    pl.seed_everything(seed)\n",
    "  \n",
    "    # Setting deterministic options so one operations so    torch.backends.cudss.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # TensorFlow seed setting\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "    except ImportError:\n",
    "        pass  # TensorFlow not installed\n",
    "    \n",
    "\n",
    "\n",
    "def save_mnist_as_parquet(dataset, path):\n",
    "    \"\"\"Save MNIST dataset as parquet file\"\"\"\n",
    "    # Prepare data and labels lists\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Convert images to pixel arrays and append labels\n",
    "    for img, label in dataset:\n",
    "        # Convert the image to a flat array\n",
    "        img_array = pd.Series(img.numpy().flatten())\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df['label'] = labels\n",
    "    \n",
    "    # Save as parquet file\n",
    "    df.to_parquet(path)\n",
    "\n",
    "\n",
    "set_seed(SEED_CONST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py (old version)\n",
    "\n",
    "# from dataclasses import dataclass, asdict\n",
    "# from typing import Optional, List, Callable\n",
    "\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "\n",
    "# PROJECT_NAME = \"test_mnist_project\"\n",
    "\n",
    "# @dataclass\n",
    "# class TrainingConfig:\n",
    "#     epochs: int = 10\n",
    "#     batch_size: int = 64\n",
    "#     gradient_accumulation_steps: int = 1\n",
    "#     clip_grad_norm: Optional[float] = 1.0\n",
    "#     seed: int = 42\n",
    "#     save_checkpoint_every: int = 10\n",
    "#     eval_every: int = 1\n",
    "\n",
    "\n",
    "#         # data_dir, \n",
    "#         # batch_size, \n",
    "#         # num_workers, \n",
    "#         # # pin_memory = True, \n",
    "#         # persistent_workers = True, \n",
    "#         # transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# @dataclass\n",
    "# class DataConfig:\n",
    "#     data_dir: str = \"./data\"\n",
    "#     batch_size: int = 64\n",
    "#     num_workers: int = 4\n",
    "#     pin_memory: bool = True\n",
    "#     image_size: int = 224\n",
    "#     channels: int = 1\n",
    "#     transform: Callable = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# @dataclass\n",
    "# class ModelConfig:\n",
    "#     model_name: str = \"simple_CNN\"\n",
    "#     pretrained: bool = False\n",
    "#     num_classes: int = 10\n",
    "#     dropout_prob: Optional[float] = 0.5\n",
    "#     input_channels: int = 1\n",
    "#     loss: nn.Module = nn.CrossEntropyLoss()\n",
    "#     in_features: int = 784\n",
    "#     out_features: int = 10_000\n",
    "\n",
    "# @dataclass\n",
    "# class OptimizerConfig:\n",
    "#     optimizer_name: str = \"adamW\"\n",
    "#     learning_rate: float = 1e-4\n",
    "#     weight_decay: float = 1e-2\n",
    "#     scheduler: Optional[str] = \"cosine\"\n",
    "#     warmup_steps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.py (trash)\n",
    "\n",
    "\n",
    "# class MetricsCalculator:\n",
    "#     def __init__(self, num_classes):\n",
    "        \n",
    "        \n",
    "#         self.metrics = {\n",
    "#             'accuracy': torchmetrics.Accuracy(task='multiclass', num_classes=num_classes),\n",
    "#             'f1_score': torchmetrics.F1Score(task='multiclass', num_classes=num_classes),\n",
    "#             'auROC': torchmetrics.AUROC(task='multiclass', num_classes=num_classes),\n",
    "#             'my_accuracy': torchmetrics.Accuracy(task='multiclass', num_classes=num_classes),\n",
    "#         }\n",
    "\n",
    "#     def update(self, preds, targets, prefix=\"\"):\n",
    "#         results = {}\n",
    "#         for name, metric in self.metrics.items():\n",
    "#             results[f\"{prefix}{name}\"] = metric(preds, targets)\n",
    "#         return results\n",
    "\n",
    "#     def reset(self):\n",
    "#         for metric in self.metrics.values():\n",
    "#             metric.reset()\n",
    "\n",
    "\n",
    "# class MyAccuracy(Metric): \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.add_module('total', default=torch.tensor(0)) # , dist_reduce_fx='sum')\n",
    "#         self.add_module('correct', default=torch.tensor(0)) # , dist_reduce_fx='sum')\n",
    "\n",
    "#     def update(self, scores, target):\n",
    "#         preds = torch.argmax(scores, dim=1)\n",
    "#         assert preds.shape == target.shape\n",
    "#         self.total += target.numel()\n",
    "#         self.correct += torch.sum(preds == target)\n",
    "\n",
    "#     def compute(self): \n",
    "#         return self.correct.float() / self.total.float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.py\n",
    "\n",
    "import torchmetrics\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class MetricsCalculator(nn.Module):\n",
    "    def __init__(self, task: str = \"MULTICLASS\", num_classes: int = 4, average: str = \"macro\", stage: str = \"train\"):\n",
    "        super().__init__()\n",
    "        self.task = \"MULTICLASS\"\n",
    "        self.num_classes = num_classes\n",
    "        self.average = average\n",
    "\n",
    "        self.metrics = nn.ModuleDict({\n",
    "            \"Accuracy\": Accuracy(task=self.task, num_classes=self.num_classes),\n",
    "            # \"F1Score\": F1Score(task=self.task, num_classes=self.num_classes, average=self.average),\n",
    "            # \"Precision\": Precision(task=self.task, num_classes=self.num_classes, average=self.average),\n",
    "            # \"Recall\": Recall(task=self.task, num_classes=self.num_classes, average=self.average),\n",
    "            # \"AUROC\": AUROC(task=self.task, num_classes=self.num_classes, average=self.average),\n",
    "        })\n",
    "\n",
    "    def update(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        for metric in self.metrics.values():\n",
    "            metric.update(preds, targets)\n",
    "\n",
    "    def compute(self) -> Dict[str, torch.Tensor]:\n",
    "        return {name: metric.compute() for name, metric in self.metrics.items()}\n",
    "\n",
    "    def reset(self):\n",
    "        for metric in self.metrics.values():\n",
    "            metric.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyAccuracy(torchmetrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds, target):\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        assert preds.shape == target.shape\n",
    "        self.correct += torch.sum(preds == target)\n",
    "        self.total += target.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.correct.float() / self.total.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, input_size, learning_rate, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.model_config = model_config\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "\n",
    "        self.model_name = model_config.model_name\n",
    "\n",
    "        self.input_channels = model_config.input_channels\n",
    "        self.num_classes = model_config.num_classes\n",
    "        self.loss = model_config.loss\n",
    "\n",
    "        self.in_features = model_config.in_features\n",
    "        self.out_features = model_config.out_features\n",
    "        self.dropout_prob = model_config.dropout_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # self.lr = learning_rate\n",
    "        # self.fc1 = nn.Linear(input_size, 50)\n",
    "        # self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=num_classes\n",
    "        )\n",
    "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: (B, 1, 28, 28)\n",
    "        output: (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        accuracy = self.accuracy(scores, y)\n",
    "        f1_score = self.f1_score(scores, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "                \"train_accuracy\": accuracy,\n",
    "                \"train_f1_score\": f1_score,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_config = ModelConfig(), \n",
    "            optimizer_cfg = OptimizerConfig(), \n",
    "        ):\n",
    "        super(LightningNet, self).__init__()\n",
    "\n",
    "        # 1. Start Initialization\n",
    "        self.model_config = model_config\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "\n",
    "        self.model_name = model_config.model_name\n",
    "\n",
    "        self.input_channels = model_config.input_channels\n",
    "        self.num_classes = model_config.num_classes\n",
    "        self.loss = model_config.loss\n",
    "\n",
    "        self.in_features = model_config.in_features\n",
    "        self.out_features = model_config.out_features\n",
    "        self.dropout_prob = model_config.dropout_prob\n",
    "\n",
    "        self.metric_task = 'task'\n",
    "        self.metrics_list = None # TODO: Now we use fix metrics\n",
    "\n",
    "        # 2. Build Model (MAIN PART!!!)\n",
    "        self._initialize_model()\n",
    "        \n",
    "        # 3. Initialize Metrics\n",
    "        metric_args = {\n",
    "            # \"metrics\": None, \n",
    "            \"task\": self.metric_task, \n",
    "            \"num_classes\": self.num_classes,\n",
    "        }\n",
    "        # self.metrics_calc_dict = {\n",
    "        #     'train': MetricsCalculator(**metric_args, stage='train'), \n",
    "        #     'val': MetricsCalculator(**metric_args, stage='val'), \n",
    "        #     'test': MetricsCalculator(**metric_args, stage='test'), \n",
    "        # }\n",
    "        self.metrics_calc_dict = {\n",
    "            'train': MyAccuracy(), # **metric_args, stage='train'), \n",
    "            'val': MyAccuracy(), # **metric_args, stage='val'), \n",
    "            'test': MyAccuracy(), # **metric_args, stage='test'), \n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_model(self): \n",
    "        \"\"\"\n",
    "        Model arthitecture here! \n",
    "        Add self.features, self.adaptive_pool, self.classifier\n",
    "        \"\"\"\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(self.input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the output size after convolutions\n",
    "        # For MNIST (28x28), after 3 max-pooling layers: 28/2/2/2 = ~3\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=self.dropout_prob),\n",
    "            nn.Linear(128, self.out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=self.dropout_prob),\n",
    "            nn.Linear(self.out_features, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: (B, 1, 28, 28)\n",
    "        output: (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _log_metrics(self, loss, metric_values, stage):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            - loss + dict[metric_name[str]: metric_value[float]]\n",
    "            - stage[str]: 'train', 'val', 'test'\n",
    "        Log metrics for the current step\n",
    "        \"\"\"\n",
    "        log_dict = {\n",
    "            f'{stage}_loss': loss,\n",
    "        }\n",
    "        log_dict.update({f'{stage}_{metric_name}': metric_value for metric_name, metric_value in metric_values.items()})\n",
    "        \n",
    "        # Log to TensorBoard\n",
    "        self.log_dict(\n",
    "            log_dict,\n",
    "            on_step=(stage == 'train'),\n",
    "            # on_epoch=True,\n",
    "            prog_bar=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def _base_step(self, batch, batch_idx, stage):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        # probs = torch.softmax(y_hat, dim=1)\n",
    "        # arg_max_logits = torch.argmax(y_hat, dim=1)\n",
    "\n",
    "        # Update and compute metrics\n",
    "        currennt_metric_calculator = self.metrics_calc_dict[stage]\n",
    "        currennt_metric_calculator(y_hat, y)\n",
    "        metric_values = {'Accuracy': currennt_metric_calculator.compute()}\n",
    "        \n",
    "        # # Update and compute metrics\n",
    "        # currennt_metric_calculator = self.metrics_calc_dict[stage]\n",
    "        # currennt_metric_calculator.update(y_hat, y)\n",
    "        # metric_values = currennt_metric_calculator.compute()\n",
    "        \n",
    "        # Log metrics\n",
    "        self._log_metrics(loss, metric_values, stage)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self, outputs):\n",
    "        self.metrics_calc_dict['train'].reset()\n",
    "        self.metrics_calc_dict['val'].reset()\n",
    "        self.metrics_calc_dict['test'].reset()\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _, _ = self._base_step(batch, batch_idx, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, _, _ = self._base_step(batch, batch_idx, 'val')\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, _, _ = self._base_step(batch, batch_idx, 'test')\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        y_hat = self(x)\n",
    "        label = torch.argmax(y_hat, dim=1)\n",
    "        return label\n",
    "    \n",
    "    def _get_optimizer(self, opt_cfg):\n",
    "        \"\"\"Create and return an optimizer based on configuration\"\"\"\n",
    "        if opt_cfg.optimizer_name.lower() == \"adamw\":\n",
    "            opt = optim.AdamW(\n",
    "                self.parameters(), \n",
    "                lr=opt_cfg.learning_rate, \n",
    "                weight_decay=opt_cfg.weight_decay\n",
    "            )\n",
    "        # elif opt_cfg.optimizer_name.lower() == \"adam\":\n",
    "        #     return optim.Adam(\n",
    "        #         self.parameters(),\n",
    "        #         lr=opt_cfg.learning_rate,\n",
    "        #         weight_decay=opt_cfg.weight_decay\n",
    "        #     )\n",
    "        # elif opt_cfg.optimizer_name.lower() == \"sgd\":\n",
    "        #     return optim.SGD(\n",
    "        #         self.parameters(),\n",
    "        #         lr=opt_cfg.learning_rate,\n",
    "        #         momentum=0.9,\n",
    "        #         weight_decay=opt_cfg.weight_decay\n",
    "        #     )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {opt_cfg.optimizer_name}\")\n",
    "\n",
    "        return opt\n",
    "\n",
    "    \n",
    "    def _get_scheduler(self, optimizer, opt_cfg):\n",
    "        \"\"\"Create and return an SCHEDULER based on configuration\"\"\"\n",
    "        if opt_cfg.scheduler is None:\n",
    "            return None  # No scheduler\n",
    "\n",
    "        if opt_cfg.scheduler.lower() == \"cosine\":\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=opt_cfg.warmup_steps\n",
    "            )\n",
    "        # elif opt_cfg.scheduler.lower() == \"steplr\":\n",
    "        #     scheduler = optim.lr_scheduler.StepLR(\n",
    "        #         optimizer,\n",
    "        #         step_size=opt_cfg.warmup_steps,\n",
    "        #         gamma=0.1\n",
    "        #     )\n",
    "        # elif opt_cfg.scheduler.lower() == \"exponential\":\n",
    "        #     scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "        #         optimizer,\n",
    "        #         gamma=0.9\n",
    "        #     )\n",
    "        # elif opt_cfg.scheduler.lower() == \"reduce_on_plateau\":\n",
    "        #     scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #         optimizer,\n",
    "        #         mode='min',\n",
    "        #         factor=0.1,\n",
    "        #         patience=10\n",
    "        #     )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scheduler: {opt_cfg.scheduler}\")\n",
    "        return scheduler\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = self._get_optimizer(self.optimizer_cfg)\n",
    "        self.scheduler = self._get_scheduler(self.optimizer, self.optimizer_cfg)\n",
    "        \n",
    "        # PyTorch Lightning expects a dictionary for schedulers\n",
    "        scheduler_config = {\n",
    "            'scheduler': self.scheduler,\n",
    "            'interval': 'epoch',  # or 'step' for step-wise updates\n",
    "            'frequency': 1,       # how often to apply scheduler.step()\n",
    "            'monitor': 'val_loss',  # metric to monitor for ReduceLROnPlateau\n",
    "            'strict': True,       # whether to crash the training if `monitor` is not found\n",
    "        }\n",
    "\n",
    "        if self.scheduler:\n",
    "            res_dict = {\n",
    "                'optimizer': self.optimizer,\n",
    "                'lr_scheduler': scheduler_config  # only if scheduler is not None\n",
    "            } \n",
    "        else: \n",
    "            return self.optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_cfg = data_cfg\n",
    "\n",
    "        self.data_dir = data_cfg.data_dir\n",
    "        self.batch_size = data_cfg.batch_size\n",
    "        self.num_workers = data_cfg.num_workers\n",
    "        # self.pin_memory = data_cfg.pin_memory\n",
    "        # self.persistent_workers = data_cfg.persistent_workers\n",
    "        self.transform = data_cfg.transform # transforms.Compose([transforms.ToTensor()])\n",
    "        self.split_percentage = data_cfg.split_percentage\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        entire_dataset = datasets.MNIST(\n",
    "            root=self.data_dir,\n",
    "            train=True,\n",
    "            transform=self.transform,\n",
    "            download=False,\n",
    "        )\n",
    "        # train_size = int(self.percentage * len(self.entire_set))\n",
    "        # val_size = len(self.entire_set) - train_size\n",
    "        # self.train_set, self.val_set = random_split(self.entire_set, [train_size, val_size])\n",
    "\n",
    "        self.train_ds, self.val_ds = random_split(entire_dataset, [50000, 10000])\n",
    "        self.test_ds = datasets.MNIST(\n",
    "            root=self.data_dir,\n",
    "            train=False,\n",
    "            transform=self.transform,\n",
    "            download=False,\n",
    "        )\n",
    "\n",
    "    def _make_dataloader(self, dataset, train_flag = True): \n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size = self.batch_size, \n",
    "            num_workers = self.num_workers, \n",
    "            persistent_workers=self.persistent_workers,\n",
    "            shuffle = train_flag\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._make_dataloader(self.train_set, train_flag=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._make_dataloader(self.val_set, train_flag=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._make_dataloader(self.test_set, train_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, List, Callable\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "PROJECT_NAME = \"test_mnist_project\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # devices: int = 1\n",
    "    min_epochs: int = 1\n",
    "    max_epochs: int = 3\n",
    "    precision: int = 16\n",
    "    accelerator: str = \"cpu\"\n",
    "\n",
    "    # Doesn't use in Lightning\n",
    "    # batch_size: int = 64 # It's aclually in the DataModule\n",
    "    # gradient_accumulation_steps: int = 1\n",
    "    # clip_grad_norm: Optional[float] = 1.0\n",
    "    # seed: int = 42\n",
    "    # save_checkpoint_every: int = 10\n",
    "    # eval_every: int = 1\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    # data cfg\n",
    "    data_dir: str = \"./data\"\n",
    "    image_size: int = 224\n",
    "    channels: int = 1\n",
    "\n",
    "    # training & data cfg\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 4\n",
    "    pin_memory: bool = True\n",
    "    persistent_workers: bool = True\n",
    "\n",
    "    split_percentage: float = 0.8\n",
    "    transform: Callable = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name: str = \"simple_CNN\"\n",
    "    pretrained: bool = False\n",
    "    num_classes: int = 10\n",
    "    dropout_prob: Optional[float] = 0.5\n",
    "    input_channels: int = 1\n",
    "    loss: nn.Module = nn.CrossEntropyLoss()\n",
    "    in_features: int = 784\n",
    "    out_features: int = 10_000\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    optimizer_name: str = \"adamW\"\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    scheduler: Optional[str] = \"cosine\"\n",
    "    warmup_steps: int = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configs\n",
    "train_cfg = TrainingConfig()\n",
    "data_cfg = DataConfig()\n",
    "model_cfg = ModelConfig()\n",
    "optimizer_cfg = OptimizerConfig()\n",
    "\n",
    "\n",
    "dm = MnistDataModule(data_cfg)\n",
    "model = LightningNet(\n",
    "    model_config = model_cfg,\n",
    "    optimizer_cfg = optimizer_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    # accelerator=\"gpu\", \n",
    "    # devices=1, \n",
    "    # min_epochs=1, \n",
    "    # max_epochs=3, \n",
    "    # precision=16\n",
    "    **asdict(train_cfg)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:183: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name          | Type              | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | loss          | CrossEntropyLoss  | 0      | train\n",
      "1 | features      | Sequential        | 93.1 K | train\n",
      "2 | adaptive_pool | AdaptiveAvgPool2d | 0      | train\n",
      "3 | classifier    | Sequential        | 1.4 M  | train\n",
      "------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.933     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9989169fc64741d9b88b0b44e1128040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, dm)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[258], line 163\u001b[0m, in \u001b[0;36mLightningNet.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 163\u001b[0m     loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_step(batch, batch_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:1154\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m   1156\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1158\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1162\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1163\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst1 = torch.tensor([1, 0, 1, 3])\n",
    "# lst2 = torch.tensor([1, 0, 1, 3])\n",
    "\n",
    "\n",
    "# class MetricsCalculator:\n",
    "#     def __init__(self, metrics: List[torchmetrics.Metric], task: str, num_classes: int, stage: str):\n",
    "#         self.metrics = {metric.__class__.__name__: metric for metric in metrics}\n",
    "#         for metric in self.metrics.values():\n",
    "#             metric.task = task\n",
    "#             metric.num_classes = num_classes\n",
    "#             metric.stage = stage\n",
    "            \n",
    "#         self.stage = stage\n",
    "\n",
    "#     def update(self, preds, targets):\n",
    "#         for metric in self.metrics.values():\n",
    "#             metric.update(preds, targets)\n",
    "\n",
    "#     def compute(self):\n",
    "#         metric_values = {}\n",
    "#         for name, metric in self.metrics.items():\n",
    "#             metric_values[name] = metric.compute()\n",
    "#         return metric_values\n",
    "\n",
    "#     def reset(self):\n",
    "#         for metric in self.metrics.values():\n",
    "#             metric.reset()\n",
    "\n",
    "# model.metrics_calc_dict['train'].update(lst1, lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst1 = [1, 0, 1, 3]\n",
    "# lst2 = [1, 0, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
